{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "universal-version",
   "metadata": {},
   "source": [
    "# [ NEW ] \n",
    "# 작사가 모델 만들기\n",
    "## Step 1. 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "genetic-lawyer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터크기 :  187088\n",
      "Examples:\n",
      " ['[Hook]', \"I've been down so long, it look like up to me\", 'They look up to me']\n"
     ]
    }
   ],
   "source": [
    "import os, glob, re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "txt_file_path = os.getenv(\"HOME\") + \"/aiffel/lyricist/data/lyrics/*\"\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "# 원본 데이터를 raw_corpus 안에 담습니다\n",
    "raw_corpus = []\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, 'r') as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "        \n",
    "print(\"데이터크기 : \", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-pontiac",
   "metadata": {},
   "source": [
    "* 샘플 데이터 10줄 정도 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "strange-integer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hook]\n",
      "I've been down so long, it look like up to me\n",
      "They look up to me\n",
      "I got fake people showin' fake love to me\n",
      "Straight up to my face, straight up to my face\n",
      "I've been down so long, it look like up to me\n",
      "They look up to me\n",
      "I got fake people showin' fake love to me\n",
      "Straight up to my face, straight up to my face [Verse 1]\n",
      "Somethin' ain't right when we talkin'\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == ':': continue\n",
    "        \n",
    "    if idx > 9: break\n",
    "    \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-constant",
   "metadata": {},
   "source": [
    "## Step 2. 데이터 정제\n",
    "1. 처리하기 까다로운 특수문자, 대문자 등의 정보를 제거함\n",
    "2. 문장 앞,뒤에 시작-종료 토큰을 추가함\n",
    "3. 효율적인 학습을 위해 지나치게 긴 문장을 제거 -> 토큰 개수 15개 미만 문장만 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "amateur-introduction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample setnence <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\"\\1\", sentence)\n",
    "    sentence = re.sub(r'[\"\"]+', \"\", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\",\" \",sentence)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = '<start> ' + sentence + ' <end>'\n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;; sample   setnence\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cross-coverage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[Hook]']\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "print(raw_corpus[0].split())\n",
    "print(len(raw_corpus[1].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "diagnostic-subscription",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start> i ve been down so long, it look like up to me <end>', '<start> they look up to me <end>', '<start> i got fake people showin fake love to me <end>', '<start> straight up to my face, straight up to my face <end>', '<start> i ve been down so long, it look like up to me <end>', '<start> they look up to me <end>', '<start> i got fake people showin fake love to me <end>', '<start> straight up to my face, straight up to my face verse <end>', '<start> somethin ain t right when we talkin <end>', '<start> somethin ain t right when we talkin <end>']\n",
      "166091\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if len(sentence.split()) >= 15: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    if sentence == \"[Hook]\": continue\n",
    "  \n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "    \n",
    "print(corpus[:10])\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "generous-trail",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# for idx, sent in enumerate(raw_corpus):\n",
    "#     if len(sent.split()) >= 15:\n",
    "# #         print(idx,':',sent)\n",
    "# #         print(idx, \":\", len(sent.split()))\n",
    "#         count += 1\n",
    "#         if count > 50: break\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "derived-saint",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(166091, 30) <keras_preprocessing.text.Tokenizer object at 0x7fde5737f5d0>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=12000,\n",
    "    filters='',\n",
    "    oov_token='<unk>'\n",
    "    )\n",
    "    \n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    \n",
    "    print(tensor.shape,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "forward-fellowship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : the\n",
      "6 : you\n",
      "7 : and\n",
      "8 : a\n",
      "9 : to\n",
      "10 : it\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "    \n",
    "    if idx >= 10: break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-indiana",
   "metadata": {},
   "source": [
    "## Step 3. 평가 데이터셋 분리\n",
    "* 단어장 크기 12000 개 이상\n",
    "* 총 데이터 20 % 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "plain-gabriel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (132872, 29)\n",
      "Target Train (132872, 29)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "src_input = tensor[:, :-1]\n",
    "\n",
    "tgt_input = tensor[:, 1:]\n",
    "\n",
    "#print(src_input[0])\n",
    "#print(tgt_input[0])\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input,\n",
    "                                                          tgt_input, \n",
    "                                                          test_size=0.2,\n",
    "                                                          random_state=42\n",
    "                                                         )\n",
    "#print(enc_train[0])\n",
    "#print(dec_train[0])\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-teacher",
   "metadata": {},
   "source": [
    "## Step 4. 모델 빌드 & 학습하기\n",
    "1. Hyperparameter : `Embedding Size`, `Hidden Size`\n",
    "2. 10 epochs 안에 val_loss 2.2 미만 수준으로 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "perfect-copying",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 29), (256, 29)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(enc_train)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(enc_train) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "preceding-guinea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 19\n",
    "hidden_size = 1024\n",
    "lyricist = TextGenerator(tokenizer.num_words + 1, embedding_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "warming-rwanda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "519/519 [==============================] - 350s 663ms/step - loss: 2.3697 - val_loss: 1.6193\n",
      "Epoch 2/10\n",
      "519/519 [==============================] - 350s 673ms/step - loss: 1.5841 - val_loss: 1.5227\n",
      "Epoch 3/10\n",
      "519/519 [==============================] - 344s 663ms/step - loss: 1.6081 - val_loss: 1.4920\n",
      "Epoch 4/10\n",
      "519/519 [==============================] - 345s 664ms/step - loss: 1.4661 - val_loss: 1.4573\n",
      "Epoch 5/10\n",
      "519/519 [==============================] - 344s 663ms/step - loss: 1.4214 - val_loss: 1.4264\n",
      "Epoch 6/10\n",
      "519/519 [==============================] - 343s 661ms/step - loss: 1.3797 - val_loss: 1.4028\n",
      "Epoch 7/10\n",
      "519/519 [==============================] - 356s 685ms/step - loss: 1.3431 - val_loss: 1.3825\n",
      "Epoch 8/10\n",
      "519/519 [==============================] - 343s 661ms/step - loss: 1.3112 - val_loss: 1.3659\n",
      "Epoch 9/10\n",
      "519/519 [==============================] - 347s 667ms/step - loss: 1.2766 - val_loss: 1.3508\n",
      "Epoch 10/10\n",
      "519/519 [==============================] - 341s 657ms/step - loss: 1.2463 - val_loss: 1.3372\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, \n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "lyricist.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "# 학습 중 loss 값 비교를 위해 history 객체로 정보 저장\n",
    "# validation 추가\n",
    "history = lyricist.fit(dataset, \n",
    "                       epochs=10,\n",
    "                       batch_size=256,\n",
    "                       validation_data=(enc_val, dec_val),\n",
    "                       verbose=1\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "instrumental-sunset",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_curve' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-53cdf6329f89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplot_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_curve' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_curve(history.epoch, history.history, ['loss', 'val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "transsexual-separate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence='<start>', max_len=20):\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index['<end>']\n",
    "    \n",
    "    while True:\n",
    "        predict = model(test_tensor)\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]\n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "            \n",
    "    generated = ''\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "        \n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "suspended-philadelphia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i need to sunroof top <end> '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(lyricist, tokenizer, init_sentence=\"<start> i need\", max_len=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-orbit",
   "metadata": {},
   "source": [
    "# [회고]\n",
    "### 알고 있던 것\n",
    "1. sequential data, 문장 토큰화\n",
    "2. 모델 요약 및 입력 데이터 사이즈 확인 -> 1 epoch 구동\n",
    "\n",
    "### 적용해 볼 점\n",
    "1. '토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습데이터에서 제외' : \n",
    "    1. 확률적으로 추론하는 과정이 길어질 수록 오차가 누적되면서 예측 성능이 떨어지는것은 어떻게 보면 당연함\n",
    "    2. 실제 프로젝트에서 짧은 예측 결과를 최대한 활용해 성능을 끌어올리는 작업이 쉽지않다.\n",
    "    \n",
    "### 새롭게 알게된 것\n",
    "1. Loss -> `Sparse Categorical entropy` :\n",
    "    1. 자연어 모델에서 one-hot 벡터 형식을 통해 Loss 값을 계산하는 것\n",
    "    2. Sparse -> [0,0,0,...,1,0,0] 예처럼 대부분 비어있는 리스트 안에 몇개의 실수 값이 들어있는 형태\n",
    "    \n",
    "### 이해가 어려운 점\n",
    "1. 정규표현식을 사용해 문장부호 필터링 전처리\n",
    "    1. 개념은 명확한데 문법이... 매번 복잡해서 자료를 확인해야하는 것이 번거로움. 외운다고 편해질까??\n",
    "2. 모델 출력 결과가 이해가 되지않는 형태로 나옴 -> start-lovelovelovelove-end... 출력값이 나오는걸로 봐선 모델 구조가 완성되고 학습도 진행된 것 같은데, 여러차례 학습 이후에도 출력값의 변화가 적음\n",
    "    1. 데이터셋을 모델에 입력하는 부분에서 채널 크기가 잘못 설정되어있는 부분을 수정함\n",
    "    \n",
    "### 전체 평가\n",
    "1. 모델 출력 형태를 보고, 케이스 별로 구분 한 이후, 모델의 최적화 작업을 진행해보고 싶었다. 다만 자연어 모델이 아직 익숙지 않아서 추가적인 활동은 추후에 시도해볼 수 있을 듯\n",
    "2. 여러 분야의 모델 지식을 한꺼번에 받아들이다보니 머릿속에서 정리하는게 쉽지 않다 -> 블로그나 깃에 정리하는 것도 간결하게 진행할 수 있으면 너무 좋을 듯\n",
    "3. `transformer` 관련해서도 깊게 공부하면 다양하게 활용할 수 있어 좋을 것 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-tragedy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
