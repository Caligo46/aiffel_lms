{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "criminal-escape",
   "metadata": {},
   "source": [
    "# [ NEW ] \n",
    "# 작사가 모델 만들기\n",
    "## Step 1. 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "attended-separation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터크기 :  187088\n",
      "Examples:\n",
      " ['[Hook]', \"I've been down so long, it look like up to me\", 'They look up to me']\n"
     ]
    }
   ],
   "source": [
    "import os, glob, re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "txt_file_path = os.getenv(\"HOME\") + \"/aiffel/lyricist/data/lyrics/*\"\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "# 원본 데이터를 raw_corpus 안에 담습니다\n",
    "raw_corpus = []\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, 'r') as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "        \n",
    "print(\"데이터크기 : \", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-consideration",
   "metadata": {},
   "source": [
    "* 샘플 데이터 10줄 정도 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fluid-buyer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hook]\n",
      "I've been down so long, it look like up to me\n",
      "They look up to me\n",
      "I got fake people showin' fake love to me\n",
      "Straight up to my face, straight up to my face\n",
      "I've been down so long, it look like up to me\n",
      "They look up to me\n",
      "I got fake people showin' fake love to me\n",
      "Straight up to my face, straight up to my face [Verse 1]\n",
      "Somethin' ain't right when we talkin'\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == ':': continue\n",
    "        \n",
    "    if idx > 9: break\n",
    "    \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-maximum",
   "metadata": {},
   "source": [
    "## Step 2. 데이터 정제\n",
    "1. 처리하기 까다로운 특수문자, 대문자 등의 정보를 제거함\n",
    "2. 문장 앞,뒤에 시작-종료 토큰을 추가함\n",
    "3. 효율적인 학습을 위해 지나치게 긴 문장을 제거 -> 토큰 개수 15개 미만 문장만 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "running-approval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample setnence <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\"\\1\", sentence)\n",
    "    sentence = re.sub(r'[\"\"]+', \"\", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\",\" \",sentence)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = '<start> ' + sentence + ' <end>'\n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;; sample   setnence\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "russian-cuisine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[Hook]']\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "print(raw_corpus[0].split())\n",
    "print(len(raw_corpus[1].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "missing-mouse",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start> i ve been down so long, it look like up to me <end>', '<start> they look up to me <end>', '<start> i got fake people showin fake love to me <end>', '<start> straight up to my face, straight up to my face <end>', '<start> i ve been down so long, it look like up to me <end>', '<start> they look up to me <end>', '<start> i got fake people showin fake love to me <end>', '<start> straight up to my face, straight up to my face verse <end>', '<start> somethin ain t right when we talkin <end>', '<start> somethin ain t right when we talkin <end>']\n",
      "166091\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if len(sentence.split()) >= 15: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    if sentence == \"[Hook]\": continue\n",
    "  \n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "    \n",
    "print(corpus[:10])\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "buried-encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# for idx, sent in enumerate(raw_corpus):\n",
    "#     if len(sent.split()) >= 15:\n",
    "# #         print(idx,':',sent)\n",
    "# #         print(idx, \":\", len(sent.split()))\n",
    "#         count += 1\n",
    "#         if count > 50: break\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "endless-strengthening",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(166091, 30) <keras_preprocessing.text.Tokenizer object at 0x7fde5737f5d0>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=12000,\n",
    "    filters='',\n",
    "    oov_token='<unk>'\n",
    "    )\n",
    "    \n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    \n",
    "    print(tensor.shape,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "harmful-round",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : the\n",
      "6 : you\n",
      "7 : and\n",
      "8 : a\n",
      "9 : to\n",
      "10 : it\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "    \n",
    "    if idx >= 10: break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-syria",
   "metadata": {},
   "source": [
    "## Step 3. 평가 데이터셋 분리\n",
    "* 단어장 크기 12000 개 이상\n",
    "* 총 데이터 20 % 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "mobile-broad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (132872, 29)\n",
      "Target Train (132872, 29)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "src_input = tensor[:, :-1]\n",
    "\n",
    "tgt_input = tensor[:, 1:]\n",
    "\n",
    "#print(src_input[0])\n",
    "#print(tgt_input[0])\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input,\n",
    "                                                          tgt_input, \n",
    "                                                          test_size=0.2,\n",
    "                                                          random_state=42\n",
    "                                                         )\n",
    "#print(enc_train[0])\n",
    "#print(dec_train[0])\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-witch",
   "metadata": {},
   "source": [
    "## Step 4. 모델 빌드 & 학습하기\n",
    "1. Hyperparameter : `Embedding Size`, `Hidden Size`\n",
    "2. 10 epochs 안에 val_loss 2.2 미만 수준으로 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "polyphonic-intervention",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 29), (256, 29)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(enc_train)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(enc_train) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faced-occurrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 19\n",
    "hidden_size = 1024\n",
    "lyricist = TextGenerator(tokenizer.num_words + 1, embedding_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "affiliated-police",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "519/519 [==============================] - 350s 663ms/step - loss: 2.3697 - val_loss: 1.6193\n",
      "Epoch 2/10\n",
      "519/519 [==============================] - 350s 673ms/step - loss: 1.5841 - val_loss: 1.5227\n",
      "Epoch 3/10\n",
      "519/519 [==============================] - 344s 663ms/step - loss: 1.6081 - val_loss: 1.4920\n",
      "Epoch 4/10\n",
      "519/519 [==============================] - 345s 664ms/step - loss: 1.4661 - val_loss: 1.4573\n",
      "Epoch 5/10\n",
      "519/519 [==============================] - 344s 663ms/step - loss: 1.4214 - val_loss: 1.4264\n",
      "Epoch 6/10\n",
      "519/519 [==============================] - 343s 661ms/step - loss: 1.3797 - val_loss: 1.4028\n",
      "Epoch 7/10\n",
      "519/519 [==============================] - 356s 685ms/step - loss: 1.3431 - val_loss: 1.3825\n",
      "Epoch 8/10\n",
      "519/519 [==============================] - 343s 661ms/step - loss: 1.3112 - val_loss: 1.3659\n",
      "Epoch 9/10\n",
      "519/519 [==============================] - 347s 667ms/step - loss: 1.2766 - val_loss: 1.3508\n",
      "Epoch 10/10\n",
      "519/519 [==============================] - 341s 657ms/step - loss: 1.2463 - val_loss: 1.3372\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, \n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "lyricist.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "# 학습 중 loss 값 비교를 위해 history 객체로 정보 저장\n",
    "# validation 추가\n",
    "history = lyricist.fit(dataset, \n",
    "                       epochs=10,\n",
    "                       batch_size=256,\n",
    "                       validation_data=(enc_val, dec_val),\n",
    "                       verbose=1\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fresh-uzbekistan",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_curve' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-53cdf6329f89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplot_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_curve' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_curve(history.epoch, history.history, ['loss', 'val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "loved-connecticut",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence='<start>', max_len=20):\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index['<end>']\n",
    "    \n",
    "    while True:\n",
    "        predict = model(test_tensor)\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]\n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "            \n",
    "    generated = ''\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "        \n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "purple-greenhouse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i need to sunroof top <end> '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(lyricist, tokenizer, init_sentence=\"<start> i need\", max_len=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-absorption",
   "metadata": {},
   "source": [
    "## 회고\n",
    "* 학습이 잘 됐다고 판단하는 근거가 필요함 -> 그리드 탐색 후 결과물 비교\n",
    "* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
